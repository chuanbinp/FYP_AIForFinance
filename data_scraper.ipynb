{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import data_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_3_FILEPATH = \"./data/data3/data3.csv\"\n",
    "DATA_4_FILEPATH = \"./data/data4/stocktwits_data_ALL.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 3\n",
    "~1200 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_SYMBOLS = ['SPY', 'DJIA', 'QQQ', 'VIX', 'BTC.X', 'ETH.X', 'XRP.X', 'ADA.X', 'SOL.X']\n",
    "\n",
    "URL = \"https://api.stocktwits.com/api/2/streams/symbol/{}.json?max={}&filter=top\"\n",
    "URL_START = \"https://api.stocktwits.com/api/2/streams/symbol/{}.json?filter=top\"\n",
    "    \n",
    "def scrape_data(symbol, target_count):\n",
    "    # All scraped data, list of dicts\n",
    "    all_messages = []\n",
    "    \n",
    "    # Initial API get request\n",
    "    url = URL_START.format(symbol)\n",
    "    response = requests.get(url)\n",
    "    content = response.json()\n",
    "    messages = content['messages']\n",
    "    last_id = process_messages(symbol, messages, all_messages)\n",
    "    print(\"{} - Scraped count: {}\".format(symbol, len(all_messages)))\n",
    "    \n",
    "    # Continue API get requests till >= target_count\n",
    "    while(len(all_messages)<target_count):\n",
    "        try:\n",
    "            url = URL.format(symbol, last_id)\n",
    "            response = requests.get(url)\n",
    "            content = response.json()\n",
    "            messages = content['messages']\n",
    "            last_id = process_messages(symbol, messages, all_messages)\n",
    "            print(\"{} - Scraped count: {}\".format(symbol, len(all_messages)))\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "    stocktwit_df = pd.DataFrame(columns=['Sentiment', \"User_id\", \"Message\", \"Date\", \"Time\", \"Symbol\"])\n",
    "    df = pd.DataFrame(all_messages)\n",
    "    stocktwit_df = pd.concat([stocktwit_df, df])\n",
    "    return stocktwit_df\n",
    "    \n",
    "def process_messages(symbol, messages, all_messages):\n",
    "    for message in messages:\n",
    "        message_dict = {}\n",
    "        \n",
    "        # Only get messages with sentiment\n",
    "        try:\n",
    "            message_dict['Sentiment'] = message['entities']['sentiment']['basic']\n",
    "        except TypeError:\n",
    "            continue\n",
    "            \n",
    "        message_dict['User_id'] = message['id']\n",
    "        message_dict['Message'] = message['body']\n",
    "        message_dict['Date'] = message['created_at'].split('T')[0]\n",
    "        message_dict['Time'] = message['created_at'].split('T')[1]\n",
    "        message_dict['Symbol'] = symbol\n",
    "        all_messages.append(message_dict)\n",
    "\n",
    "    last_id = str(messages[-1]['id'])\n",
    "    return last_id\n",
    "\n",
    "\n",
    "stocktwit_df = pd.DataFrame(columns=['Sentiment', \"User_id\", \"Message\", \"Date\", \"Time\", \"Symbol\"])\n",
    "\n",
    "for symbol in LIST_OF_SYMBOLS:\n",
    "    curr_df = scrape_data(symbol, 200)\n",
    "    stocktwit_df = pd.concat([stocktwit_df, curr_df])\n",
    "    \n",
    "stocktwit_df.to_csv(DATA_3_FILEPATH, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After annotating..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8554340112983937"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_3_FILEPATH_CLEANED = \"./data/data3/data3_final.csv\"\n",
    "\n",
    "data3 = pd.read_csv(DATA_3_FILEPATH_CLEANED)\n",
    "cohen_kappa_score([1 if senti==\"Bullish\" else 0 for senti in data3[\"Sentiment\"]], data3[\"Annotator1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_SYMBOLS = ['SPY', 'DJIA', 'QQQ', 'VIX', 'BTC.X', 'ETH.X', 'XRP.X', 'ADA.X', 'SOL.X']\n",
    "\n",
    "URL = \"https://api.stocktwits.com/api/2/streams/symbol/{}.json?max={}&filter=top\"\n",
    "URL_START = \"https://api.stocktwits.com/api/2/streams/symbol/{}.json?filter=top\"\n",
    "\n",
    "def save_to_csv(df, symbol):\n",
    "    filename_csv = \"./scrape_data/stocktwits_data_\"+symbol+\".csv\"\n",
    "    df.to_csv(filename_csv, index = False)\n",
    "    print(\"File saved as: \", filename_csv)\n",
    "    \n",
    "def scrape_data(symbol, target_count):\n",
    "    # All scraped data, list of dicts\n",
    "    all_messages = []\n",
    "    \n",
    "    # Initial API get request\n",
    "    url = URL_START.format(symbol)\n",
    "    response = requests.get(url)\n",
    "    content = response.json()\n",
    "    messages = content['messages']\n",
    "    last_id = process_messages(symbol, messages, all_messages)\n",
    "    print(\"{} - Scraped count: {}\".format(symbol, len(all_messages)))\n",
    "    \n",
    "    prev_high = 0\n",
    "    # Continue API get requests till >= target_count\n",
    "    while(len(all_messages)<target_count):\n",
    "        try:\n",
    "            url = URL.format(symbol, last_id)\n",
    "            response = requests.get(url)\n",
    "            content = response.json()\n",
    "            messages = content['messages']\n",
    "            last_id = process_messages(symbol, messages, all_messages)\n",
    "\n",
    "            if(len(all_messages)//1000 > prev_high):\n",
    "                print(\"{} - Scraped count: {}\".format(symbol, len(all_messages)))\n",
    "                prev_high = len(all_messages)//1000\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    print(\"{} - Scraped count: {}\".format(symbol, len(all_messages)))\n",
    "    stocktwit_df = pd.DataFrame(columns=['Sentiment', \"User_id\", \"Message\", \"Date\", \"Time\", \"Symbol\"])\n",
    "    df = pd.DataFrame(all_messages)\n",
    "    stocktwit_df = pd.concat([stocktwit_df, df])\n",
    "    save_to_csv(stocktwit_df, symbol)\n",
    "    \n",
    "def process_messages(symbol, messages, all_messages):\n",
    "    for message in messages:\n",
    "        message_dict = {}\n",
    "        \n",
    "        # Only get messages with sentiment\n",
    "        try:\n",
    "            message_dict['Sentiment'] = message['entities']['sentiment']['basic']\n",
    "        except TypeError:\n",
    "            continue\n",
    "            \n",
    "        message_dict['User_id'] = message['id']\n",
    "        message_dict['Message'] = message['body']\n",
    "        message_dict['Date'] = message['created_at'].split('T')[0]\n",
    "        message_dict['Time'] = message['created_at'].split('T')[1]\n",
    "        message_dict['Symbol'] = symbol\n",
    "        all_messages.append(message_dict)\n",
    "\n",
    "    last_id = str(messages[-1]['id'])\n",
    "    return last_id\n",
    "\n",
    "\n",
    "stocktwit_df = pd.DataFrame(columns=['Sentiment', \"User_id\", \"Message\", \"Date\", \"Time\", \"Symbol\"])\n",
    "\n",
    "for symbol in LIST_OF_SYMBOLS:\n",
    "    scrape_data(symbol, 110000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocktwit_df = pd.DataFrame(columns=['Sentiment', \"User_id\", \"Message\", \"Date\", \"Time\", \"Symbol\"])\n",
    "\n",
    "for symbol in LIST_OF_SYMBOLS:\n",
    "    curr_df = pd.read_csv(\"./scrape_data/stocktwits_data_\"+symbol+\".csv\")\n",
    "    stocktwit_df = pd.concat([stocktwit_df, curr_df])\n",
    "    \n",
    "stocktwit_df = stocktwit_df.drop_duplicates(subset='Message')\n",
    "stocktwit_df.to_csv(DATA_4_FILEPATH, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean data and save to csv, since file is huge..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_4_FILEPATH_CLEANED = \"./data/data4/stocktwits_data_ALL_cleaned.csv\"\n",
    "\n",
    "data4 = pd.read_csv(DATA_4_FILEPATH)\n",
    "data4 = data_reader.preprocess_data(data4, \"Message\")\n",
    "data4.to_csv(DATA_4_FILEPATH_CLEANED, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
