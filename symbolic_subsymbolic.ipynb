{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\phoec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\phoec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\phoec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\phoec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from afinn import Afinn\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import custom_lexicons.senticnet.senticnet as sentic\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import data_reader\n",
    "import results_analyser\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_DIR = \"./custom_lexicons/\"\n",
    "\n",
    "NTUSD_FILEPATH = \"ntusd/NTUSD_Fin_word_v1.0.json\"\n",
    "STOCKTWITLEXI_FILEPATH = \"stocktwitlexi/domain_lexicon_raw_norm.csv\"\n",
    "SENTI_DD_FILEPATH = \"sentidd/Senti-DD_data1.csv\"\n",
    "LM_FILEPATH = \"sentidd/LM_Word_List.csv\"\n",
    "\n",
    "# AFINN\n",
    "afinn = Afinn()\n",
    "\n",
    "# VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# NTUSD-FIN\n",
    "with open(FILE_DIR+NTUSD_FILEPATH, \"r\") as f:\n",
    "    data = f.read()\n",
    "    NTUSD = json.loads(data)\n",
    "word_sent_dict = {}\n",
    "for i in range(len(NTUSD)):\n",
    "    word_sent_dict[NTUSD[i][\"token\"]] = NTUSD[i][\"market_sentiment\"]\n",
    "    \n",
    "# STOCKTWITLEXI    \n",
    "stocktwitlexi = pd.read_csv(FILE_DIR+STOCKTWITLEXI_FILEPATH, header=None, index_col=0)\n",
    "stocktwitlexi = stocktwitlexi.to_dict()[1]\n",
    "\n",
    "# SENTIDD\n",
    "sentidd = pd.read_csv(FILE_DIR+SENTI_DD_FILEPATH)\n",
    "sentidd_dict = dict(zip(zip(sentidd.entity, sentidd.directional_word), sentidd.sentiment))\n",
    "lm_df = pd.read_csv(FILE_DIR+LM_FILEPATH)\n",
    "lm_dict = dict(zip(lm_df.word, lm_df.label))\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_stopwords(data):\n",
    "    sentence_token = [s.split(' ') for s in data] \n",
    "    return sentence_token\n",
    "\n",
    "def get_wordnet_tag(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_scores(score, count):\n",
    "    if (count>0):\n",
    "        return score/count\n",
    "    else:\n",
    "        return score\n",
    "\n",
    "def standardise_scores(pred_raw, lexicon):\n",
    "    if(lexicon==\"senticnet\"): #-1 to 1\n",
    "        return pred_raw\n",
    "    elif(lexicon==\"ntusd\"): #-3.81 to 1.22, range is 5 so /2.5\n",
    "        return [pred/2.5 for pred in pred_raw]\n",
    "    elif(lexicon==\"sentiwordnet\"): #-1 to 1 since pos-neg\n",
    "        return pred_raw\n",
    "    elif(lexicon==\"stocktwitlexi\"): #-1 to 1\n",
    "        return pred_raw\n",
    "    elif(lexicon==\"afinn\"): #-5 to 5\n",
    "        return [pred/5 for pred in pred_raw]\n",
    "    elif(lexicon==\"vader\"): #-1 to 1\n",
    "        return pred_raw\n",
    "    elif(lexicon==\"sentidd\"): #-2 to 2\n",
    "        return [pred/2 for pred in pred_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senti_dd_polarity(text, sentidd_dict, lm_dict):\n",
    "    def lm_score(text, lm_dict):\n",
    "        tokens = word_tokenize(text)\n",
    "        count = 0\n",
    "        score = 0\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                if lm_dict[token]==\"positive\":\n",
    "                    score += 1\n",
    "                    count += 1\n",
    "                elif lm_dict[token]==\"negative\":\n",
    "                    score -= 1\n",
    "                    count += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return score/count if count>0 else score\n",
    "\n",
    "    def senti_dd_score(text, sentidd_dict):\n",
    "        tokens = word_tokenize(text)\n",
    "        count = 0\n",
    "        score = 0\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        for stemmed_token in stemmed_tokens:\n",
    "            for lemmatized_token in lemmatized_tokens:\n",
    "                \n",
    "                try:\n",
    "                    if (sentidd_dict[(lemmatized_token, stemmed_token)] ==\"positive\"):\n",
    "                        score += 1\n",
    "                        count += 1\n",
    "                    elif (sentidd_dict[(lemmatized_token, stemmed_token)] ==\"negative\"):\n",
    "                        score -= 1\n",
    "                        count += 1\n",
    "                except:\n",
    "                    pass\n",
    "        return score/count if count>0 else score\n",
    "    \n",
    "    score = lm_score(text, lm_dict)\n",
    "    context_sentiment_score = senti_dd_score(text, sentidd_dict)\n",
    "    if context_sentiment_score > 0: score += 1\n",
    "    elif context_sentiment_score < 0: score -= 1\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_scoring(X, y_class):\n",
    "    ntusd_pred_raw = []\n",
    "    stocktwitlexi_pred_raw = []\n",
    "    afinn_pred_raw = []\n",
    "    vader_pred_raw = []\n",
    "    sentidd_pred_raw = []\n",
    "    \n",
    "    for test_tweet in X:\n",
    "        ntusd_score = 0\n",
    "        stocktwitlexi_score = 0\n",
    "        afinn_score = 0\n",
    "        \n",
    "        ntusd_count = 0\n",
    "        stocktwitlexi_count = 0\n",
    "        afinn_count = 0\n",
    "        \n",
    "        sentence_tagged = np.array(nltk.pos_tag(test_tweet))\n",
    "        for tagged in sentence_tagged:\n",
    "            word = tagged[0]\n",
    "            wn_tag = get_wordnet_tag(tagged[1])\n",
    "            \n",
    "            #NTUSD\n",
    "            try: \n",
    "                ntusd_score += word_sent_dict[word]\n",
    "                ntusd_count += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            #Stocktwitlexi\n",
    "            try: \n",
    "                stocktwitlexi_score += stocktwitlexi[word]\n",
    "                stocktwitlexi_count += 1\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            #Afinn\n",
    "            try: \n",
    "                afinn_score += afinn.score(word)\n",
    "                afinn_count += 1\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        s = \" \".join(test_tweet)\n",
    "        \n",
    "        #Vader\n",
    "        vader_output = analyzer.polarity_scores(s)\n",
    "        vader_score = vader_output[\"compound\"] \n",
    "        \n",
    "        #Senti-DD\n",
    "        sentidd_score = senti_dd_polarity(s, sentidd_dict, lm_dict)\n",
    "        \n",
    "        ntusd_score = normalise_scores(ntusd_score, ntusd_count)\n",
    "        stocktwitlexi_score = normalise_scores(stocktwitlexi_score, stocktwitlexi_count)\n",
    "        afinn_score = normalise_scores(afinn_score, afinn_count)\n",
    "        vader_score = vader_score #already normalised\n",
    "        sentidd_score = sentidd_score #already normalised\n",
    "        \n",
    "        ntusd_pred_raw.append(ntusd_score)\n",
    "        stocktwitlexi_pred_raw.append(stocktwitlexi_score)\n",
    "        afinn_pred_raw.append(afinn_score)\n",
    "        vader_pred_raw.append(vader_score)\n",
    "        sentidd_pred_raw.append(sentidd_score)\n",
    "        \n",
    "    ntusd_pred = standardise_scores(ntusd_pred_raw, \"ntusd\")\n",
    "    stocktwitlexi_pred = standardise_scores(stocktwitlexi_pred_raw, \"stocktwitlexi\")\n",
    "    afinn_pred = standardise_scores(afinn_pred_raw, \"afinn\")\n",
    "    vader_pred = standardise_scores(vader_pred_raw, \"vader\")\n",
    "    sentidd_pred = standardise_scores(sentidd_pred_raw, \"sentidd\")\n",
    "\n",
    "    combined_df = pd.DataFrame()\n",
    "    combined_df['ntusd'] = ntusd_pred\n",
    "    combined_df['afinn'] = afinn_pred\n",
    "    combined_df['vader'] = vader_pred\n",
    "    combined_df['stocktwitlexi'] = stocktwitlexi_pred\n",
    "    combined_df['sentidd'] = sentidd_pred\n",
    "    combined_df['actual_class'] = y_class\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_voting_leave2soft(row):\n",
    "    lowest_col = row.sort_values().idxmin()\n",
    "    highest_col = row.sort_values().idxmax()\n",
    "\n",
    "    return row.sort_values().iloc[1:4].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsymbolic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# check if it is running with GPU or not\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to load the data\n",
    "def data_handler(root, train_filename, dev_filename):\n",
    "    train_data_raw = pd.read_csv(root + train_filename)\n",
    "    val_data_raw = pd.read_csv(root + dev_filename)\n",
    "    X_train, y_train = train_data_raw['text_cleaned'], train_data_raw['Label']\n",
    "    X_val, y_val = val_data_raw['text_cleaned'], val_data_raw['Label']\n",
    "    # concatenate the train and validation set text_cleaned data to get the token ids for the words\n",
    "    frames = [X_train, X_val]\n",
    "    X = pd.concat(frames)\n",
    "    # convert from pandas dataframe to numpy arrays\n",
    "    X_ = X.to_numpy()\n",
    "    return X_train, y_train, X_val, y_val, X_\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,             # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# tokenize data\n",
    "# Specify `MAX_LEN`\n",
    "MAX_LEN = 256\n",
    "\n",
    "def create_data_loader(root,train_filename,dev_filename,batch_size=16):\n",
    "    X_train, y_train, X_val, y_val, X_ = data_handler(root, train_filename, dev_filename)\n",
    "\n",
    "    # Concatenate train data and test data\n",
    "    all_texts = np.concatenate([X_train, X_val])\n",
    "\n",
    "    # Encode our concatenated data\n",
    "    encoded_texts = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_texts]\n",
    "\n",
    "    # Find the maximum length\n",
    "    max_len = max([len(sent) for sent in encoded_texts])\n",
    "\n",
    "    # Print sentence 0 and its encoded token ids\n",
    "    token_ids = list(preprocessing_for_bert([X_[0]])[0].squeeze().numpy())\n",
    "\n",
    "    # Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "#     print('Tokenizing data...')\n",
    "    train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "    val_inputs, val_masks = preprocessing_for_bert(X_val)\n",
    "\n",
    "    # Convert other data types to torch.Tensor\n",
    "    train_labels = torch.tensor(y_train)\n",
    "    val_labels = torch.tensor(y_val)\n",
    "\n",
    "    # Create the DataLoader for our training set\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create the DataLoader for our validation set\n",
    "    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "def initialize_model(train_dataloader, epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler\n",
    "\n",
    "# ROC curve and AUC value to test the value on the validation set\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results(data, model, symbolic_weightage, results_df):\n",
    "    \n",
    "    #Symbolic\n",
    "    if(data==\"data1\"):\n",
    "        X, y_class = data_reader.read_data1(\"list\")\n",
    "    elif(data==\"data2\"):\n",
    "        X, y_class = data_reader.read_data2(\"list\")\n",
    "    elif(data==\"data3\"):\n",
    "        X, y_class = data_reader.read_data3(\"list\")\n",
    "        \n",
    "    combined_df = individual_scoring(X, y_class)\n",
    "    combination = [\"ntusd\", \"stocktwitlexi\", \"afinn\", \"vader\", \"sentidd\"]\n",
    "    y_class = combined_df['actual_class']\n",
    "    curr_df = combined_df[combination]\n",
    "    voting_leave2soft = curr_df.apply(combine_voting_leave2soft, axis=1)\n",
    "    voting_leave2soft = voting_leave2soft.to_list()\n",
    "    \n",
    "    #Subsymbolic\n",
    "    batch_size = 16\n",
    "    test_filepath = data + \"_test.csv\"\n",
    "    _, val_loader, y_class = create_data_loader(root=DATA_PATH,\n",
    "                                          train_filename=test_filepath,\n",
    "                                          dev_filename=test_filepath,\n",
    "                                          batch_size=batch_size)\n",
    "    bert_probs = bert_predict(model, val_loader)\n",
    "    bert_probs = [(2*prob[1])-1 for prob in bert_probs] #convert to -1 to 1\n",
    "\n",
    "    #Combine\n",
    "    overall_probs = [(x*symbolic_weightage)+(y*(1-symbolic_weightage)) for x,y in zip(voting_leave2soft, bert_probs)]\n",
    "    pred_class = results_analyser.probability_to_class(overall_probs)\n",
    "    \n",
    "    return results_analyser.calculate_metrics(results_df, y_class, pred_class, data+\"_symbolic\"+str(+symbolic_weightage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\phoec\\anaconda3\\envs\\tf2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\phoec\\anaconda3\\envs\\tf2\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "DATA_PATH = cwd+\"/data/subsymbolic/\"\n",
    "SAVED_MODEL_PATH = cwd+\"/model/\"\n",
    "\n",
    "combined_bert_model = \"bert_model_combined_data.pt\"\n",
    "combined_train_filepath = \"combined_data_train.csv\"\n",
    "combined_val_filepath = \"combined_data_val.csv\"\n",
    "\n",
    "batch_size = 16\n",
    "epo = 5\n",
    "\n",
    "# load the data\n",
    "train_loader, val_loader, y_val_data1 = create_data_loader(root=DATA_PATH,\n",
    "                                                     train_filename=combined_train_filepath,\n",
    "                                                     dev_filename=combined_val_filepath,\n",
    "                                                     batch_size=batch_size)\n",
    "\n",
    "# call the saved weights\n",
    "PATH = SAVED_MODEL_PATH+combined_bert_model\n",
    "model, _, _ = initialize_model(train_loader,epochs=epo)\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Data 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phoec\\anaconda3\\envs\\tf2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "C:\\Users\\phoec\\anaconda3\\envs\\tf2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "C:\\Users\\phoec\\anaconda3\\envs\\tf2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "C:\\Users\\phoec\\anaconda3\\envs\\tf2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.752000</td>\n",
       "      <td>data2_symbolic0.2</td>\n",
       "      <td>0.813734</td>\n",
       "      <td>0.742947</td>\n",
       "      <td>0.899431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.761143</td>\n",
       "      <td>data2_symbolic0.4</td>\n",
       "      <td>0.820908</td>\n",
       "      <td>0.748437</td>\n",
       "      <td>0.908918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.773714</td>\n",
       "      <td>data2_symbolic0.6</td>\n",
       "      <td>0.831058</td>\n",
       "      <td>0.755039</td>\n",
       "      <td>0.924099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.794286</td>\n",
       "      <td>data2_symbolic0.8</td>\n",
       "      <td>0.845626</td>\n",
       "      <td>0.771518</td>\n",
       "      <td>0.935484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy         Experiment  F1_score  Precision    Recall\n",
       "0  0.752000  data2_symbolic0.2  0.813734   0.742947  0.899431\n",
       "1  0.761143  data2_symbolic0.4  0.820908   0.748437  0.908918\n",
       "2  0.773714  data2_symbolic0.6  0.831058   0.755039  0.924099\n",
       "3  0.794286  data2_symbolic0.8  0.845626   0.771518  0.935484"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phoec\\anaconda3\\envs\\tf2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "C:\\Users\\phoec\\anaconda3\\envs\\tf2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "C:\\Users\\phoec\\anaconda3\\envs\\tf2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "C:\\Users\\phoec\\anaconda3\\envs\\tf2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.734711</td>\n",
       "      <td>data3_symbolic0.2</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.729529</td>\n",
       "      <td>0.867257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.737295</td>\n",
       "      <td>data3_symbolic0.4</td>\n",
       "      <td>0.794058</td>\n",
       "      <td>0.732254</td>\n",
       "      <td>0.867257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.744186</td>\n",
       "      <td>data3_symbolic0.6</td>\n",
       "      <td>0.799189</td>\n",
       "      <td>0.737828</td>\n",
       "      <td>0.871681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.756245</td>\n",
       "      <td>data3_symbolic0.8</td>\n",
       "      <td>0.807089</td>\n",
       "      <td>0.750317</td>\n",
       "      <td>0.873156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy         Experiment  F1_score  Precision    Recall\n",
       "0  0.734711  data3_symbolic0.2  0.792453   0.729529  0.867257\n",
       "1  0.737295  data3_symbolic0.4  0.794058   0.732254  0.867257\n",
       "2  0.744186  data3_symbolic0.6  0.799189   0.737828  0.871681\n",
       "3  0.756245  data3_symbolic0.8  0.807089   0.750317  0.873156"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_data2 = pd.DataFrame()\n",
    "results_data3 = pd.DataFrame()\n",
    "\n",
    "symbolic_weights = [0.2,0.4,0.6,0.8]\n",
    "for p in symbolic_weights:\n",
    "    results_data2 = generate_results(\"data2\", model, p, results_data2)\n",
    "    \n",
    "display(results_data2)\n",
    "\n",
    "\n",
    "for p in symbolic_weights:\n",
    "    results_data3 = generate_results(\"data3\", model, p, results_data3)\n",
    "    \n",
    "display(results_data3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FILE_DIR = \"./results/\"\n",
    "\n",
    "results_data2.to_csv(RESULTS_FILE_DIR+\"result2_df_hybrid.csv\", index=False)\n",
    "results_data3.to_csv(RESULTS_FILE_DIR+\"result3_df_hybrid.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
