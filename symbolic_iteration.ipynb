{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\phoec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\phoec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\phoec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from afinn import Afinn\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import custom_lexicons.senticnet.senticnet as sentic\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import data_reader\n",
    "import results_analyser\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_DIR = \"./custom_lexicons/\"\n",
    "\n",
    "NTUSD_FILEPATH = \"ntusd/NTUSD_Fin_word_v1.0.json\"\n",
    "STOCKTWITLEXI_FILEPATH = \"stocktwitlexi/domain_lexicon_raw_norm.csv\"\n",
    "SENTI_DD_FILEPATH = \"sentidd/sentidd_data1.csv\"\n",
    "LM_FILEPATH = \"sentidd/LM_Word_List.csv\"\n",
    "\n",
    "# AFINN\n",
    "afinn = Afinn()\n",
    "\n",
    "# VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# NTUSD-FIN\n",
    "with open(FILE_DIR+NTUSD_FILEPATH, \"r\") as f:\n",
    "    data = f.read()\n",
    "    NTUSD = json.loads(data)\n",
    "word_sent_dict = {}\n",
    "for i in range(len(NTUSD)):\n",
    "    word_sent_dict[NTUSD[i][\"token\"]] = NTUSD[i][\"market_sentiment\"]\n",
    "    \n",
    "# STOCKTWITLEXI    \n",
    "stocktwitlexi = pd.read_csv(FILE_DIR+STOCKTWITLEXI_FILEPATH, header=None, index_col=0)\n",
    "stocktwitlexi = stocktwitlexi.to_dict()[1]\n",
    "\n",
    "# SENTIDD\n",
    "sentidd = pd.read_csv(FILE_DIR+SENTI_DD_FILEPATH)\n",
    "sentidd_dict = dict(zip(zip(sentidd.entity, sentidd.directional_word), sentidd.sentiment))\n",
    "lm_df = pd.read_csv(FILE_DIR+LM_FILEPATH)\n",
    "lm_dict = dict(zip(lm_df.word, lm_df.label))\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_stopwords(data):\n",
    "    sentence_token = [s.split(' ') for s in data] \n",
    "    return sentence_token\n",
    "\n",
    "def get_wordnet_tag(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_scores(score, count):\n",
    "    if (count>0):\n",
    "        return score/count\n",
    "    else:\n",
    "        return score\n",
    "\n",
    "def normalise_scores(pred_raw, lexicon):\n",
    "    if(lexicon==\"senticnet\"): #-1 to 1\n",
    "        return pred_raw\n",
    "    elif(lexicon==\"ntusd\"): #-3.81 to 1.22, range is 5 so /2.5\n",
    "        return [pred/2.5 for pred in pred_raw]\n",
    "    elif(lexicon==\"sentiwordnet\"): #-1 to 1 since pos-neg\n",
    "        return pred_raw\n",
    "    elif(lexicon==\"stocktwitlexi\"): #-1 to 1\n",
    "        return pred_raw\n",
    "    elif(lexicon==\"afinn\"): #-5 to 5\n",
    "        return [pred/5 for pred in pred_raw]\n",
    "    elif(lexicon==\"vader\"): #-1 to 1\n",
    "        return pred_raw\n",
    "    elif(lexicon==\"sentidd\"): #-2 to 2\n",
    "        return [pred/2 for pred in pred_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senti_dd_polarity(text, sentidd_dict, lm_dict):\n",
    "    def lm_score(text, lm_dict):\n",
    "        tokens = word_tokenize(text)\n",
    "        count = 0\n",
    "        score = 0\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                if lm_dict[token]==\"positive\":\n",
    "                    score += 1\n",
    "                    count += 1\n",
    "                elif lm_dict[token]==\"negative\":\n",
    "                    score -= 1\n",
    "                    count += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return score/count if count>0 else score\n",
    "\n",
    "    def senti_dd_score(text, sentidd_dict):\n",
    "        tokens = word_tokenize(text)\n",
    "        count = 0\n",
    "        score = 0\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        for stemmed_token in stemmed_tokens:\n",
    "            for lemmatized_token in lemmatized_tokens:\n",
    "                \n",
    "                try:\n",
    "                    if (sentidd_dict[(lemmatized_token, stemmed_token)] ==\"positive\"):\n",
    "                        score += 1\n",
    "                        count += 1\n",
    "                    elif (sentidd_dict[(lemmatized_token, stemmed_token)] ==\"negative\"):\n",
    "                        score -= 1\n",
    "                        count += 1\n",
    "                except:\n",
    "                    pass\n",
    "        return score/count if count>0 else score\n",
    "    \n",
    "    score = lm_score(text, lm_dict)\n",
    "    context_sentiment_score = senti_dd_score(text, sentidd_dict)\n",
    "    if context_sentiment_score > 0: score += 1\n",
    "    elif context_sentiment_score < 0: score -= 1\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_scoring(X, y_class):\n",
    "    sentic_pred_raw = []\n",
    "    ntusd_pred_raw = []\n",
    "    sentiwordnet_pred_raw = []\n",
    "    stocktwitlexi_pred_raw = []\n",
    "    afinn_pred_raw = []\n",
    "    vader_pred_raw = []\n",
    "    sentidd_pred_raw = []\n",
    "    \n",
    "    for test_tweet in X:\n",
    "        sentic_score = 0\n",
    "        ntusd_score = 0\n",
    "        sentiwordnet_score = 0\n",
    "        stocktwitlexi_score = 0\n",
    "        afinn_score = 0\n",
    "        \n",
    "        sentic_count = 0\n",
    "        ntusd_count = 0\n",
    "        sentiwordnet_count = 0\n",
    "        stocktwitlexi_count = 0\n",
    "        afinn_count = 0\n",
    "        \n",
    "        sentence_tagged = np.array(nltk.pos_tag(test_tweet))\n",
    "        for tagged in sentence_tagged:\n",
    "            word = tagged[0]\n",
    "            wn_tag = get_wordnet_tag(tagged[1])\n",
    "            \n",
    "            #Senticnet\n",
    "            try: \n",
    "                sentic_score += sentic.senticnet[word][7]\n",
    "                sentic_count += 1\n",
    "            except:\n",
    "                pass\n",
    "            #NTUSD\n",
    "            try: \n",
    "                ntusd_score += word_sent_dict[word]\n",
    "                ntusd_count += 1\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            #Senticwordnet\n",
    "            if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV,  wn.VERB):            \n",
    "                lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "                if lemma:\n",
    "                    synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "                    if synsets:\n",
    "                        swn_synset = swn.senti_synset(synsets[0].name())\n",
    "                        sentiwordnet_score += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "                        sentiwordnet_count += 1\n",
    "            #Stocktwitlexi\n",
    "            try: \n",
    "                stocktwitlexi_score += stocktwitlexi[word]\n",
    "                stocktwitlexi_count += 1\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            #Afinn\n",
    "            try: \n",
    "                afinn_score += afinn.score(word)\n",
    "                afinn_count += 1\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        #Afinn\n",
    "        s = \" \".join(test_tweet)\n",
    "        \n",
    "        #Vader\n",
    "        vader_output = analyzer.polarity_scores(s)\n",
    "        vader_score = vader_output[\"compound\"] #vader_output[\"pos\"] - vader_output[\"neg\"]\n",
    "        \n",
    "        #Senti-DD\n",
    "        sentidd_score = senti_dd_polarity(s, sentidd_dict, lm_dict)\n",
    "        \n",
    "        sentic_score = standardise_scores(sentic_score, sentic_count)\n",
    "        ntusd_score = standardise_scores(ntusd_score, ntusd_count)\n",
    "        sentiwordnet_score = standardise_scores(sentiwordnet_score, sentiwordnet_count)\n",
    "        stocktwitlexi_score = standardise_scores(stocktwitlexi_score, stocktwitlexi_count)\n",
    "        afinn_score = standardise_scores(afinn_score, afinn_count)\n",
    "        vader_score = vader_score #already normalised\n",
    "        sentidd_score = sentidd_score #already normalised\n",
    "        \n",
    "        sentic_pred_raw.append(sentic_score)\n",
    "        ntusd_pred_raw.append(ntusd_score)\n",
    "        sentiwordnet_pred_raw.append(sentiwordnet_score)\n",
    "        stocktwitlexi_pred_raw.append(stocktwitlexi_score)\n",
    "        afinn_pred_raw.append(afinn_score)\n",
    "        vader_pred_raw.append(vader_score)\n",
    "        sentidd_pred_raw.append(sentidd_score)\n",
    "        \n",
    "    sentic_pred = normalise_scores(sentic_pred_raw, \"senticnet\")\n",
    "    ntusd_pred = normalise_scores(ntusd_pred_raw, \"ntusd\")\n",
    "    sentiwordnet_pred = normalise_scores(sentiwordnet_pred_raw, \"sentiwordnet\")\n",
    "    stocktwitlexi_pred = normalise_scores(stocktwitlexi_pred_raw, \"stocktwitlexi\")\n",
    "    afinn_pred = normalise_scores(afinn_pred_raw, \"afinn\")\n",
    "    vader_pred = normalise_scores(vader_pred_raw, \"vader\")\n",
    "    sentidd_pred = normalise_scores(sentidd_pred_raw, \"sentidd\")\n",
    "\n",
    "    combined_df = pd.DataFrame()\n",
    "    combined_df['senticnet'] = sentic_pred\n",
    "    combined_df['ntusd'] = ntusd_pred\n",
    "    combined_df['sentiwordnet'] = sentiwordnet_pred\n",
    "    combined_df['afinn'] = afinn_pred\n",
    "    combined_df['vader'] = vader_pred\n",
    "    combined_df['stocktwitlexi'] = stocktwitlexi_pred\n",
    "    combined_df['sentidd'] = sentidd_pred\n",
    "    combined_df['actual_class'] = y_class\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_voting_leave2soft(row):\n",
    "    lowest_col = row.sort_values().idxmin()\n",
    "    highest_col = row.sort_values().idxmax()\n",
    "    lowest_dict[lowest_col] += 1\n",
    "    highest_dict[highest_col] += 1\n",
    "    return row.sort_values().iloc[1:4].mean()\n",
    "\n",
    "def combine_voting_soft(row):\n",
    "    return row.sort_values().mean()\n",
    "\n",
    "def combine_voting_hard(row):\n",
    "    pos_vote = (row > 0).sum()\n",
    "    neg_vote = (row <= 0).sum()\n",
    "    return 1 if pos_vote>neg_vote else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results(combined_df):\n",
    "    combination_list = []\n",
    "    lexicons = [\"senticnet\", \"ntusd\", \"sentiwordnet\", \"stocktwitlexi\", \"afinn\", \"vader\", \"sentidd\"]\n",
    "\n",
    "    for i in range(1, len(lexicons)+1):\n",
    "        combination_tuples = list(itertools.combinations(lexicons, i))\n",
    "        combination_list.extend([list(elem) for elem in combination_tuples])\n",
    "\n",
    "    y_class = combined_df['actual_class']\n",
    "    results_df = pd.DataFrame()\n",
    "\n",
    "    for combination in combination_list:\n",
    "        curr_df = combined_df[combination]\n",
    "\n",
    "        if(len(combination)>1): #more than 1\n",
    "            voting_soft = curr_df.apply(combine_voting_soft, axis=1)\n",
    "            pred_class = results_analyser.probability_to_class(voting_soft)\n",
    "            results_df = results_analyser.calculate_metrics(results_df, y_class, pred_class, '_'.join(combination)+\"_soft\")\n",
    "\n",
    "            if(len(combination)%2==1): #odd number\n",
    "                voting_hard = curr_df.apply(combine_voting_hard, axis=1)\n",
    "                results_df = results_analyser.calculate_metrics(results_df, y_class, voting_hard, '_'.join(combination)+\"_hard\")\n",
    "\n",
    "            if(len(combination)>=5 and len(combination)%2==1): #odd and gte 5\n",
    "                voting_leave2soft = curr_df.apply(combine_voting_leave2soft, axis=1)\n",
    "                pred_class = results_analyser.probability_to_class(voting_leave2soft)\n",
    "                results_df = results_analyser.calculate_metrics(results_df, y_class, pred_class, '_'.join(combination)+\"_leave2soft\")\n",
    "        else:\n",
    "            pred_class = results_analyser.probability_to_class(combined_df[combination[0]])\n",
    "            results_df = results_analyser.calculate_metrics(results_df, y_class, pred_class, '_'.join(combination))\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_X, data1_y_class = data_reader.read_data1(\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>senticnet</th>\n",
       "      <th>ntusd</th>\n",
       "      <th>sentiwordnet</th>\n",
       "      <th>afinn</th>\n",
       "      <th>vader</th>\n",
       "      <th>stocktwitlexi</th>\n",
       "      <th>sentidd</th>\n",
       "      <th>actual_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8300</td>\n",
       "      <td>-0.383587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.070687</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6160</td>\n",
       "      <td>0.242586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>0.137161</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.0750</td>\n",
       "      <td>-0.021231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.4497</td>\n",
       "      <td>0.094244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1825</td>\n",
       "      <td>0.031240</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>0.104018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8100</td>\n",
       "      <td>0.129776</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>0.083150</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0.9065</td>\n",
       "      <td>-0.052211</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.128948</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>-0.0895</td>\n",
       "      <td>-0.236600</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.7717</td>\n",
       "      <td>0.098817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>0.8090</td>\n",
       "      <td>0.127908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.097468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>0.5555</td>\n",
       "      <td>-0.015682</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.082139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>0.1786</td>\n",
       "      <td>0.023368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.5574</td>\n",
       "      <td>0.066062</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2023 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      senticnet     ntusd  sentiwordnet     afinn   vader  stocktwitlexi  \\\n",
       "0        0.8300 -0.383587      0.000000  0.000000  0.0000       0.070687   \n",
       "1        0.6160  0.242586      0.000000  0.060000  0.5267       0.137161   \n",
       "2       -0.0750 -0.021231      0.000000  0.040000  0.4497       0.094244   \n",
       "3        0.1825  0.031240      0.022727  0.000000  0.6310       0.104018   \n",
       "4        0.8100  0.129776      0.145833  0.040000  0.4404       0.083150   \n",
       "...         ...       ...           ...       ...     ...            ...   \n",
       "2018     0.9065 -0.052211      0.041667  0.000000  0.0000       0.128948   \n",
       "2019    -0.0895 -0.236600      0.125000  0.111111  0.7717       0.098817   \n",
       "2020     0.8090  0.127908      0.000000  0.000000  0.0000       0.097468   \n",
       "2021     0.5555 -0.015682     -0.062500  0.000000  0.0000       0.082139   \n",
       "2022     0.1786  0.023368      0.000000  0.028571  0.5574       0.066062   \n",
       "\n",
       "      sentidd  actual_class  \n",
       "0        -0.5             0  \n",
       "1        -0.5             1  \n",
       "2         0.0             1  \n",
       "3         0.0             1  \n",
       "4         0.5             1  \n",
       "...       ...           ...  \n",
       "2018      0.0             1  \n",
       "2019      0.0             0  \n",
       "2020      0.0             1  \n",
       "2021      0.0             1  \n",
       "2022     -0.5             0  \n",
       "\n",
       "[2023 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined1_df = individual_scoring(data1_X, data1_y_class)\n",
    "combined1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_dict = {key: 0 for key in combined1_df.columns}\n",
    "highest_dict = {key: 0 for key in combined1_df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'senticnet': 6619, 'ntusd': 6304, 'sentiwordnet': 8541, 'afinn': 7789, 'vader': 5433, 'stocktwitlexi': 1559, 'sentidd': 8261, 'actual_class': 0}\n",
      "{'senticnet': 16871, 'ntusd': 6507, 'sentiwordnet': 1983, 'afinn': 1013, 'vader': 7395, 'stocktwitlexi': 7576, 'sentidd': 3161, 'actual_class': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.618389</td>\n",
       "      <td>senticnet</td>\n",
       "      <td>0.715968</td>\n",
       "      <td>0.691542</td>\n",
       "      <td>0.742182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.709343</td>\n",
       "      <td>ntusd</td>\n",
       "      <td>0.782383</td>\n",
       "      <td>0.759885</td>\n",
       "      <td>0.806255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.566485</td>\n",
       "      <td>sentiwordnet</td>\n",
       "      <td>0.609006</td>\n",
       "      <td>0.732833</td>\n",
       "      <td>0.520976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.695502</td>\n",
       "      <td>stocktwitlexi</td>\n",
       "      <td>0.805923</td>\n",
       "      <td>0.686527</td>\n",
       "      <td>0.975591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.506179</td>\n",
       "      <td>afinn</td>\n",
       "      <td>0.504218</td>\n",
       "      <td>0.721591</td>\n",
       "      <td>0.387490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.670292</td>\n",
       "      <td>senticnet_sentiwordnet_stocktwitlexi_afinn_vad...</td>\n",
       "      <td>0.760158</td>\n",
       "      <td>0.719048</td>\n",
       "      <td>0.806255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.722689</td>\n",
       "      <td>ntusd_sentiwordnet_stocktwitlexi_afinn_vader_s...</td>\n",
       "      <td>0.794430</td>\n",
       "      <td>0.764457</td>\n",
       "      <td>0.826850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.680178</td>\n",
       "      <td>senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...</td>\n",
       "      <td>0.768183</td>\n",
       "      <td>0.724324</td>\n",
       "      <td>0.817696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.611962</td>\n",
       "      <td>senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...</td>\n",
       "      <td>0.664099</td>\n",
       "      <td>0.756335</td>\n",
       "      <td>0.591915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.618389</td>\n",
       "      <td>senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...</td>\n",
       "      <td>0.659312</td>\n",
       "      <td>0.782199</td>\n",
       "      <td>0.569794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy                                         Experiment  F1_score  \\\n",
       "0    0.618389                                          senticnet  0.715968   \n",
       "1    0.709343                                              ntusd  0.782383   \n",
       "2    0.566485                                       sentiwordnet  0.609006   \n",
       "3    0.695502                                      stocktwitlexi  0.805923   \n",
       "4    0.506179                                              afinn  0.504218   \n",
       "..        ...                                                ...       ...   \n",
       "201  0.670292  senticnet_sentiwordnet_stocktwitlexi_afinn_vad...  0.760158   \n",
       "202  0.722689  ntusd_sentiwordnet_stocktwitlexi_afinn_vader_s...  0.794430   \n",
       "203  0.680178  senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...  0.768183   \n",
       "204  0.611962  senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...  0.664099   \n",
       "205  0.618389  senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...  0.659312   \n",
       "\n",
       "     Precision    Recall  \n",
       "0     0.691542  0.742182  \n",
       "1     0.759885  0.806255  \n",
       "2     0.732833  0.520976  \n",
       "3     0.686527  0.975591  \n",
       "4     0.721591  0.387490  \n",
       "..         ...       ...  \n",
       "201   0.719048  0.806255  \n",
       "202   0.764457  0.826850  \n",
       "203   0.724324  0.817696  \n",
       "204   0.756335  0.591915  \n",
       "205   0.782199  0.569794  \n",
       "\n",
       "[206 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result1_df = generate_results(combined1_df)\n",
    "print(lowest_dict)\n",
    "print(highest_dict)\n",
    "display(result1_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2_X, data2_y_class = data_reader.read_data2(\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>senticnet</th>\n",
       "      <th>ntusd</th>\n",
       "      <th>sentiwordnet</th>\n",
       "      <th>afinn</th>\n",
       "      <th>vader</th>\n",
       "      <th>stocktwitlexi</th>\n",
       "      <th>sentidd</th>\n",
       "      <th>actual_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.302600</td>\n",
       "      <td>-0.084072</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.038095</td>\n",
       "      <td>0.5859</td>\n",
       "      <td>0.062336</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.126786</td>\n",
       "      <td>0.131360</td>\n",
       "      <td>0.017045</td>\n",
       "      <td>-0.005263</td>\n",
       "      <td>0.2023</td>\n",
       "      <td>0.088886</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.137750</td>\n",
       "      <td>0.026968</td>\n",
       "      <td>-0.013158</td>\n",
       "      <td>-0.024242</td>\n",
       "      <td>-0.7351</td>\n",
       "      <td>0.088475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.857500</td>\n",
       "      <td>0.286495</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.3818</td>\n",
       "      <td>0.089604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.854500</td>\n",
       "      <td>0.309721</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0.116167</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>0.802500</td>\n",
       "      <td>0.152355</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.088337</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>0.661000</td>\n",
       "      <td>0.088727</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0.082826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>0.850500</td>\n",
       "      <td>0.170936</td>\n",
       "      <td>-0.057692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.040192</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>0.833000</td>\n",
       "      <td>-0.152019</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.2023</td>\n",
       "      <td>0.049296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>0.271000</td>\n",
       "      <td>0.026862</td>\n",
       "      <td>-0.009615</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>-0.018969</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>875 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     senticnet     ntusd  sentiwordnet     afinn   vader  stocktwitlexi  \\\n",
       "0     0.302600 -0.084072      0.015625  0.038095  0.5859       0.062336   \n",
       "1    -0.126786  0.131360      0.017045 -0.005263  0.2023       0.088886   \n",
       "2     0.137750  0.026968     -0.013158 -0.024242 -0.7351       0.088475   \n",
       "3     0.857500  0.286495      0.020833  0.018182  0.3818       0.089604   \n",
       "4     0.854500  0.309721      0.015625  0.040000  0.6249       0.116167   \n",
       "..         ...       ...           ...       ...     ...            ...   \n",
       "870   0.802500  0.152355     -0.050000  0.020000  0.2732       0.088337   \n",
       "871   0.661000  0.088727      0.022727  0.030769  0.5719       0.082826   \n",
       "872   0.850500  0.170936     -0.057692  0.000000  0.0000       0.040192   \n",
       "873   0.833000 -0.152019      0.041667  0.080000  0.2023       0.049296   \n",
       "874   0.271000  0.026862     -0.009615  0.008696  0.2960      -0.018969   \n",
       "\n",
       "     sentidd  actual_class  \n",
       "0        0.5             1  \n",
       "1       -0.5             0  \n",
       "2        0.0             1  \n",
       "3        0.0             1  \n",
       "4        0.5             1  \n",
       "..       ...           ...  \n",
       "870      0.5             1  \n",
       "871      0.0             1  \n",
       "872     -0.5             1  \n",
       "873      0.0             1  \n",
       "874     -0.5             0  \n",
       "\n",
       "[875 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined2_df = individual_scoring(data2_X, data2_y_class)\n",
    "combined2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_dict = {key: 0 for key in combined2_df.columns}\n",
    "highest_dict = {key: 0 for key in combined2_df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'senticnet': 1753, 'ntusd': 2551, 'sentiwordnet': 3328, 'afinn': 3449, 'vader': 2736, 'stocktwitlexi': 490, 'sentidd': 4943, 'actual_class': 0}\n",
      "{'senticnet': 7454, 'ntusd': 1836, 'sentiwordnet': 561, 'afinn': 223, 'vader': 4644, 'stocktwitlexi': 2725, 'sentidd': 1807, 'actual_class': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.668571</td>\n",
       "      <td>senticnet</td>\n",
       "      <td>0.760726</td>\n",
       "      <td>0.672993</td>\n",
       "      <td>0.874763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.662857</td>\n",
       "      <td>ntusd</td>\n",
       "      <td>0.739629</td>\n",
       "      <td>0.691419</td>\n",
       "      <td>0.795066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.635429</td>\n",
       "      <td>sentiwordnet</td>\n",
       "      <td>0.691787</td>\n",
       "      <td>0.704724</td>\n",
       "      <td>0.679317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.620571</td>\n",
       "      <td>stocktwitlexi</td>\n",
       "      <td>0.755882</td>\n",
       "      <td>0.617047</td>\n",
       "      <td>0.975332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.717714</td>\n",
       "      <td>afinn</td>\n",
       "      <td>0.740818</td>\n",
       "      <td>0.828638</td>\n",
       "      <td>0.669829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.798857</td>\n",
       "      <td>senticnet_sentiwordnet_stocktwitlexi_afinn_vad...</td>\n",
       "      <td>0.847487</td>\n",
       "      <td>0.779904</td>\n",
       "      <td>0.927894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.817143</td>\n",
       "      <td>ntusd_sentiwordnet_stocktwitlexi_afinn_vader_s...</td>\n",
       "      <td>0.855072</td>\n",
       "      <td>0.818024</td>\n",
       "      <td>0.895636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...</td>\n",
       "      <td>0.849268</td>\n",
       "      <td>0.777603</td>\n",
       "      <td>0.935484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.766857</td>\n",
       "      <td>senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...</td>\n",
       "      <td>0.813528</td>\n",
       "      <td>0.784832</td>\n",
       "      <td>0.844402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.785143</td>\n",
       "      <td>senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.834320</td>\n",
       "      <td>0.802657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy                                         Experiment  F1_score  \\\n",
       "0    0.668571                                          senticnet  0.760726   \n",
       "1    0.662857                                              ntusd  0.739629   \n",
       "2    0.635429                                       sentiwordnet  0.691787   \n",
       "3    0.620571                                      stocktwitlexi  0.755882   \n",
       "4    0.717714                                              afinn  0.740818   \n",
       "..        ...                                                ...       ...   \n",
       "201  0.798857  senticnet_sentiwordnet_stocktwitlexi_afinn_vad...  0.847487   \n",
       "202  0.817143  ntusd_sentiwordnet_stocktwitlexi_afinn_vader_s...  0.855072   \n",
       "203  0.800000  senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...  0.849268   \n",
       "204  0.766857  senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...  0.813528   \n",
       "205  0.785143  senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...  0.818182   \n",
       "\n",
       "     Precision    Recall  \n",
       "0     0.672993  0.874763  \n",
       "1     0.691419  0.795066  \n",
       "2     0.704724  0.679317  \n",
       "3     0.617047  0.975332  \n",
       "4     0.828638  0.669829  \n",
       "..         ...       ...  \n",
       "201   0.779904  0.927894  \n",
       "202   0.818024  0.895636  \n",
       "203   0.777603  0.935484  \n",
       "204   0.784832  0.844402  \n",
       "205   0.834320  0.802657  \n",
       "\n",
       "[206 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result2_df = generate_results(combined2_df)\n",
    "print(lowest_dict)\n",
    "print(highest_dict)\n",
    "display(result2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3_X, data3_y_class = data_reader.read_data3(\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>senticnet</th>\n",
       "      <th>ntusd</th>\n",
       "      <th>sentiwordnet</th>\n",
       "      <th>afinn</th>\n",
       "      <th>vader</th>\n",
       "      <th>stocktwitlexi</th>\n",
       "      <th>sentidd</th>\n",
       "      <th>actual_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.931000</td>\n",
       "      <td>-0.061594</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.095693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.303667</td>\n",
       "      <td>-0.191655</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2023</td>\n",
       "      <td>0.087639</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020500</td>\n",
       "      <td>-0.300220</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>-0.183496</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.244225</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>-0.080000</td>\n",
       "      <td>-0.6115</td>\n",
       "      <td>0.040343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.358000</td>\n",
       "      <td>0.159665</td>\n",
       "      <td>-0.033333</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.045528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>0.446667</td>\n",
       "      <td>-0.015374</td>\n",
       "      <td>-0.093750</td>\n",
       "      <td>-0.027273</td>\n",
       "      <td>-0.0783</td>\n",
       "      <td>0.095814</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>0.024500</td>\n",
       "      <td>-0.425183</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.088961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>0.472500</td>\n",
       "      <td>-0.472243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.3182</td>\n",
       "      <td>-0.134641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>0.879000</td>\n",
       "      <td>-0.260604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.090518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.171002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>0.115206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1161 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      senticnet     ntusd  sentiwordnet     afinn   vader  stocktwitlexi  \\\n",
       "0      0.931000 -0.061594      0.312500  0.000000  0.0000       0.095693   \n",
       "1      0.303667 -0.191655      0.015625  0.000000  0.2023       0.087639   \n",
       "2      0.020500 -0.300220      0.150000  0.066667  0.0772      -0.183496   \n",
       "3      0.000000 -0.244225      0.013889 -0.080000 -0.6115       0.040343   \n",
       "4      0.358000  0.159665     -0.033333  0.009524  0.0000       0.045528   \n",
       "...         ...       ...           ...       ...     ...            ...   \n",
       "1156   0.446667 -0.015374     -0.093750 -0.027273 -0.0783       0.095814   \n",
       "1157   0.024500 -0.425183     -0.062500  0.000000  0.0000       0.088961   \n",
       "1158   0.472500 -0.472243      0.000000 -0.100000 -0.3182      -0.134641   \n",
       "1159   0.879000 -0.260604      0.000000  0.000000  0.0000       0.090518   \n",
       "1160   0.450000  0.171002      0.000000  0.050000  0.3612       0.115206   \n",
       "\n",
       "      sentidd  actual_class  \n",
       "0         0.0             1  \n",
       "1         0.5             0  \n",
       "2         0.5             0  \n",
       "3         0.0             0  \n",
       "4         0.0             0  \n",
       "...       ...           ...  \n",
       "1156     -0.5             0  \n",
       "1157      0.0             0  \n",
       "1158      0.0             0  \n",
       "1159      0.0             0  \n",
       "1160      0.0             0  \n",
       "\n",
       "[1161 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined3_df = individual_scoring(data3_X, data3_y_class)\n",
    "combined3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_dict = {key: 0 for key in combined3_df.columns}\n",
    "highest_dict = {key: 0 for key in combined3_df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'senticnet': 3888, 'ntusd': 4979, 'sentiwordnet': 4024, 'afinn': 3621, 'vader': 3848, 'stocktwitlexi': 813, 'sentidd': 4369, 'actual_class': 0}\n",
      "{'senticnet': 9109, 'ntusd': 2946, 'sentiwordnet': 1530, 'afinn': 567, 'vader': 4201, 'stocktwitlexi': 5205, 'sentidd': 1984, 'actual_class': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.584841</td>\n",
       "      <td>senticnet</td>\n",
       "      <td>0.668956</td>\n",
       "      <td>0.625964</td>\n",
       "      <td>0.718289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.761413</td>\n",
       "      <td>ntusd</td>\n",
       "      <td>0.790943</td>\n",
       "      <td>0.809892</td>\n",
       "      <td>0.772861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.558140</td>\n",
       "      <td>sentiwordnet</td>\n",
       "      <td>0.585956</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.535398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.638243</td>\n",
       "      <td>stocktwitlexi</td>\n",
       "      <td>0.755530</td>\n",
       "      <td>0.624038</td>\n",
       "      <td>0.957227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.538329</td>\n",
       "      <td>afinn</td>\n",
       "      <td>0.508257</td>\n",
       "      <td>0.672330</td>\n",
       "      <td>0.408555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.640827</td>\n",
       "      <td>senticnet_sentiwordnet_stocktwitlexi_afinn_vad...</td>\n",
       "      <td>0.713008</td>\n",
       "      <td>0.668387</td>\n",
       "      <td>0.764012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.703704</td>\n",
       "      <td>ntusd_sentiwordnet_stocktwitlexi_afinn_vader_s...</td>\n",
       "      <td>0.747059</td>\n",
       "      <td>0.744868</td>\n",
       "      <td>0.749263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...</td>\n",
       "      <td>0.729937</td>\n",
       "      <td>0.692715</td>\n",
       "      <td>0.771386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.608958</td>\n",
       "      <td>senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...</td>\n",
       "      <td>0.622924</td>\n",
       "      <td>0.712928</td>\n",
       "      <td>0.553097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.622739</td>\n",
       "      <td>senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...</td>\n",
       "      <td>0.613074</td>\n",
       "      <td>0.764317</td>\n",
       "      <td>0.511799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy                                         Experiment  F1_score  \\\n",
       "0    0.584841                                          senticnet  0.668956   \n",
       "1    0.761413                                              ntusd  0.790943   \n",
       "2    0.558140                                       sentiwordnet  0.585956   \n",
       "3    0.638243                                      stocktwitlexi  0.755530   \n",
       "4    0.538329                                              afinn  0.508257   \n",
       "..        ...                                                ...       ...   \n",
       "201  0.640827  senticnet_sentiwordnet_stocktwitlexi_afinn_vad...  0.713008   \n",
       "202  0.703704  ntusd_sentiwordnet_stocktwitlexi_afinn_vader_s...  0.747059   \n",
       "203  0.666667  senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...  0.729937   \n",
       "204  0.608958  senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...  0.622924   \n",
       "205  0.622739  senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...  0.613074   \n",
       "\n",
       "     Precision    Recall  \n",
       "0     0.625964  0.718289  \n",
       "1     0.809892  0.772861  \n",
       "2     0.647059  0.535398  \n",
       "3     0.624038  0.957227  \n",
       "4     0.672330  0.408555  \n",
       "..         ...       ...  \n",
       "201   0.668387  0.764012  \n",
       "202   0.744868  0.749263  \n",
       "203   0.692715  0.771386  \n",
       "204   0.712928  0.553097  \n",
       "205   0.764317  0.511799  \n",
       "\n",
       "[206 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result3_df = generate_results(combined3_df)\n",
    "print(lowest_dict)\n",
    "print(highest_dict)\n",
    "display(result3_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FILE_DIR = \"./results/\"\n",
    "\n",
    "result1_df.to_csv(RESULTS_FILE_DIR+\"result1_df.csv\", index=False)\n",
    "result2_df.to_csv(RESULTS_FILE_DIR+\"result2_df.csv\", index=False)\n",
    "result3_df.to_csv(RESULTS_FILE_DIR+\"result3_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>sentiwordnet_stocktwitlexi_afinn_soft</td>\n",
       "      <td>0.801070</td>\n",
       "      <td>0.713095</td>\n",
       "      <td>0.913806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.710826</td>\n",
       "      <td>senticnet_ntusd_stocktwitlexi_afinn_vader_leav...</td>\n",
       "      <td>0.801223</td>\n",
       "      <td>0.722426</td>\n",
       "      <td>0.899314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.720217</td>\n",
       "      <td>senticnet_ntusd_stocktwitlexi_vader_sentidd_le...</td>\n",
       "      <td>0.802650</td>\n",
       "      <td>0.739242</td>\n",
       "      <td>0.877956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.737024</td>\n",
       "      <td>ntusd_sentiwordnet_afinn_soft</td>\n",
       "      <td>0.802817</td>\n",
       "      <td>0.780822</td>\n",
       "      <td>0.826087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.708848</td>\n",
       "      <td>senticnet_ntusd_stocktwitlexi_hard</td>\n",
       "      <td>0.803993</td>\n",
       "      <td>0.713105</td>\n",
       "      <td>0.921434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.724172</td>\n",
       "      <td>ntusd_sentiwordnet_stocktwitlexi_hard</td>\n",
       "      <td>0.804485</td>\n",
       "      <td>0.744005</td>\n",
       "      <td>0.875667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.732575</td>\n",
       "      <td>ntusd_sentiwordnet_stocktwitlexi_vader_sentidd...</td>\n",
       "      <td>0.805605</td>\n",
       "      <td>0.761549</td>\n",
       "      <td>0.855072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.695502</td>\n",
       "      <td>stocktwitlexi</td>\n",
       "      <td>0.805923</td>\n",
       "      <td>0.686527</td>\n",
       "      <td>0.975591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.697973</td>\n",
       "      <td>stocktwitlexi_afinn_soft</td>\n",
       "      <td>0.806829</td>\n",
       "      <td>0.688985</td>\n",
       "      <td>0.973303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.733070</td>\n",
       "      <td>ntusd_stocktwitlexi_vader_soft</td>\n",
       "      <td>0.809725</td>\n",
       "      <td>0.752456</td>\n",
       "      <td>0.876430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.734058</td>\n",
       "      <td>ntusd_stocktwitlexi_afinn_vader_soft</td>\n",
       "      <td>0.810028</td>\n",
       "      <td>0.754109</td>\n",
       "      <td>0.874905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.736530</td>\n",
       "      <td>ntusd_sentiwordnet_stocktwitlexi_afinn_sentidd...</td>\n",
       "      <td>0.810118</td>\n",
       "      <td>0.760027</td>\n",
       "      <td>0.867277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.723678</td>\n",
       "      <td>senticnet_ntusd_stocktwitlexi_afinn_sentidd_le...</td>\n",
       "      <td>0.810573</td>\n",
       "      <td>0.729268</td>\n",
       "      <td>0.912281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.734553</td>\n",
       "      <td>ntusd_sentiwordnet_stocktwitlexi_afinn_vader_l...</td>\n",
       "      <td>0.811513</td>\n",
       "      <td>0.751625</td>\n",
       "      <td>0.881770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.738013</td>\n",
       "      <td>ntusd_sentiwordnet_stocktwitlexi_vader_soft</td>\n",
       "      <td>0.812190</td>\n",
       "      <td>0.758438</td>\n",
       "      <td>0.874142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.739001</td>\n",
       "      <td>ntusd_sentiwordnet_stocktwitlexi_afinn_vader_soft</td>\n",
       "      <td>0.812367</td>\n",
       "      <td>0.760479</td>\n",
       "      <td>0.871854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.737024</td>\n",
       "      <td>ntusd_stocktwitlexi_soft</td>\n",
       "      <td>0.820027</td>\n",
       "      <td>0.736778</td>\n",
       "      <td>0.924485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.745922</td>\n",
       "      <td>ntusd_stocktwitlexi_afinn_soft</td>\n",
       "      <td>0.824932</td>\n",
       "      <td>0.745231</td>\n",
       "      <td>0.923722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.750371</td>\n",
       "      <td>ntusd_sentiwordnet_stocktwitlexi_soft</td>\n",
       "      <td>0.825682</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>0.912281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.756797</td>\n",
       "      <td>ntusd_sentiwordnet_stocktwitlexi_afinn_soft</td>\n",
       "      <td>0.829640</td>\n",
       "      <td>0.759670</td>\n",
       "      <td>0.913806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy                                         Experiment  F1_score  \\\n",
       "78   0.705882              sentiwordnet_stocktwitlexi_afinn_soft  0.801070   \n",
       "153  0.710826  senticnet_ntusd_stocktwitlexi_afinn_vader_leav...  0.801223   \n",
       "159  0.720217  senticnet_ntusd_stocktwitlexi_vader_sentidd_le...  0.802650   \n",
       "60   0.737024                      ntusd_sentiwordnet_afinn_soft  0.802817   \n",
       "31   0.708848                 senticnet_ntusd_stocktwitlexi_hard  0.803993   \n",
       "59   0.724172              ntusd_sentiwordnet_stocktwitlexi_hard  0.804485   \n",
       "186  0.732575  ntusd_sentiwordnet_stocktwitlexi_vader_sentidd...  0.805605   \n",
       "3    0.695502                                      stocktwitlexi  0.805923   \n",
       "22   0.697973                           stocktwitlexi_afinn_soft  0.806829   \n",
       "68   0.733070                     ntusd_stocktwitlexi_vader_soft  0.809725   \n",
       "124  0.734058               ntusd_stocktwitlexi_afinn_vader_soft  0.810028   \n",
       "183  0.736530  ntusd_sentiwordnet_stocktwitlexi_afinn_sentidd...  0.810118   \n",
       "156  0.723678  senticnet_ntusd_stocktwitlexi_afinn_sentidd_le...  0.810573   \n",
       "180  0.734553  ntusd_sentiwordnet_stocktwitlexi_afinn_vader_l...  0.811513   \n",
       "119  0.738013        ntusd_sentiwordnet_stocktwitlexi_vader_soft  0.812190   \n",
       "178  0.739001  ntusd_sentiwordnet_stocktwitlexi_afinn_vader_soft  0.812367   \n",
       "14   0.737024                           ntusd_stocktwitlexi_soft  0.820027   \n",
       "66   0.745922                     ntusd_stocktwitlexi_afinn_soft  0.824932   \n",
       "58   0.750371              ntusd_sentiwordnet_stocktwitlexi_soft  0.825682   \n",
       "118  0.756797        ntusd_sentiwordnet_stocktwitlexi_afinn_soft  0.829640   \n",
       "\n",
       "     Precision    Recall  \n",
       "78    0.713095  0.913806  \n",
       "153   0.722426  0.899314  \n",
       "159   0.739242  0.877956  \n",
       "60    0.780822  0.826087  \n",
       "31    0.713105  0.921434  \n",
       "59    0.744005  0.875667  \n",
       "186   0.761549  0.855072  \n",
       "3     0.686527  0.975591  \n",
       "22    0.688985  0.973303  \n",
       "68    0.752456  0.876430  \n",
       "124   0.754109  0.874905  \n",
       "183   0.760027  0.867277  \n",
       "156   0.729268  0.912281  \n",
       "180   0.751625  0.881770  \n",
       "119   0.758438  0.874142  \n",
       "178   0.760479  0.871854  \n",
       "14    0.736778  0.924485  \n",
       "66    0.745231  0.923722  \n",
       "58    0.754098  0.912281  \n",
       "118   0.759670  0.913806  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1_df.iloc[result1_df['F1_score'].sort_values().index].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.798857</td>\n",
       "      <td>senticnet_sentiwordnet_stocktwitlexi_afinn_vad...</td>\n",
       "      <td>0.847487</td>\n",
       "      <td>0.779904</td>\n",
       "      <td>0.927894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.797714</td>\n",
       "      <td>senticnet_ntusd_sentiwordnet_stocktwitlexi_vad...</td>\n",
       "      <td>0.847807</td>\n",
       "      <td>0.775157</td>\n",
       "      <td>0.935484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.802286</td>\n",
       "      <td>senticnet_ntusd_vader_sentidd_soft</td>\n",
       "      <td>0.847845</td>\n",
       "      <td>0.790164</td>\n",
       "      <td>0.914611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.802286</td>\n",
       "      <td>senticnet_ntusd_afinn_vader_sentidd_soft</td>\n",
       "      <td>0.847845</td>\n",
       "      <td>0.790164</td>\n",
       "      <td>0.914611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.798857</td>\n",
       "      <td>senticnet_ntusd_stocktwitlexi_afinn_vader_sent...</td>\n",
       "      <td>0.848537</td>\n",
       "      <td>0.776378</td>\n",
       "      <td>0.935484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...</td>\n",
       "      <td>0.849268</td>\n",
       "      <td>0.777603</td>\n",
       "      <td>0.935484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.808000</td>\n",
       "      <td>senticnet_ntusd_sentiwordnet_afinn_vader_senti...</td>\n",
       "      <td>0.852373</td>\n",
       "      <td>0.793781</td>\n",
       "      <td>0.920304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.808000</td>\n",
       "      <td>senticnet_ntusd_sentiwordnet_vader_sentidd_soft</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.792822</td>\n",
       "      <td>0.922201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.817143</td>\n",
       "      <td>ntusd_sentiwordnet_stocktwitlexi_afinn_vader_s...</td>\n",
       "      <td>0.855072</td>\n",
       "      <td>0.818024</td>\n",
       "      <td>0.895636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.817143</td>\n",
       "      <td>ntusd_sentiwordnet_stocktwitlexi_vader_sentidd...</td>\n",
       "      <td>0.855335</td>\n",
       "      <td>0.816926</td>\n",
       "      <td>0.897533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.819429</td>\n",
       "      <td>ntusd_stocktwitlexi_vader_sentidd_soft</td>\n",
       "      <td>0.856624</td>\n",
       "      <td>0.820870</td>\n",
       "      <td>0.895636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.820571</td>\n",
       "      <td>ntusd_stocktwitlexi_afinn_vader_sentidd_soft</td>\n",
       "      <td>0.857402</td>\n",
       "      <td>0.822300</td>\n",
       "      <td>0.895636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.827429</td>\n",
       "      <td>ntusd_vader_sentidd_soft</td>\n",
       "      <td>0.858746</td>\n",
       "      <td>0.846863</td>\n",
       "      <td>0.870968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.829714</td>\n",
       "      <td>ntusd_afinn_vader_sentidd_soft</td>\n",
       "      <td>0.860617</td>\n",
       "      <td>0.848708</td>\n",
       "      <td>0.872865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.826286</td>\n",
       "      <td>stocktwitlexi_vader_sentidd_soft</td>\n",
       "      <td>0.862568</td>\n",
       "      <td>0.823834</td>\n",
       "      <td>0.905123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.827429</td>\n",
       "      <td>stocktwitlexi_afinn_vader_sentidd_soft</td>\n",
       "      <td>0.863595</td>\n",
       "      <td>0.824138</td>\n",
       "      <td>0.907021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.828571</td>\n",
       "      <td>sentiwordnet_stocktwitlexi_vader_sentidd_soft</td>\n",
       "      <td>0.863884</td>\n",
       "      <td>0.827826</td>\n",
       "      <td>0.903226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.833143</td>\n",
       "      <td>ntusd_sentiwordnet_afinn_vader_sentidd_soft</td>\n",
       "      <td>0.864060</td>\n",
       "      <td>0.848263</td>\n",
       "      <td>0.880455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.828571</td>\n",
       "      <td>sentiwordnet_stocktwitlexi_afinn_vader_sentidd...</td>\n",
       "      <td>0.864130</td>\n",
       "      <td>0.826690</td>\n",
       "      <td>0.905123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.835429</td>\n",
       "      <td>ntusd_sentiwordnet_vader_sentidd_soft</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.851376</td>\n",
       "      <td>0.880455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy                                         Experiment  F1_score  \\\n",
       "201  0.798857  senticnet_sentiwordnet_stocktwitlexi_afinn_vad...  0.847487   \n",
       "198  0.797714  senticnet_ntusd_sentiwordnet_stocktwitlexi_vad...  0.847807   \n",
       "107  0.802286                 senticnet_ntusd_vader_sentidd_soft  0.847845   \n",
       "160  0.802286           senticnet_ntusd_afinn_vader_sentidd_soft  0.847845   \n",
       "200  0.798857  senticnet_ntusd_stocktwitlexi_afinn_vader_sent...  0.848537   \n",
       "203  0.800000  senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...  0.849268   \n",
       "199  0.808000  senticnet_ntusd_sentiwordnet_afinn_vader_senti...  0.852373   \n",
       "148  0.808000    senticnet_ntusd_sentiwordnet_vader_sentidd_soft  0.852632   \n",
       "202  0.817143  ntusd_sentiwordnet_stocktwitlexi_afinn_vader_s...  0.855072   \n",
       "184  0.817143  ntusd_sentiwordnet_stocktwitlexi_vader_sentidd...  0.855335   \n",
       "126  0.819429             ntusd_stocktwitlexi_vader_sentidd_soft  0.856624   \n",
       "190  0.820571       ntusd_stocktwitlexi_afinn_vader_sentidd_soft  0.857402   \n",
       "76   0.827429                           ntusd_vader_sentidd_soft  0.858746   \n",
       "127  0.829714                     ntusd_afinn_vader_sentidd_soft  0.860617   \n",
       "94   0.826286                   stocktwitlexi_vader_sentidd_soft  0.862568   \n",
       "132  0.827429             stocktwitlexi_afinn_vader_sentidd_soft  0.863595   \n",
       "130  0.828571      sentiwordnet_stocktwitlexi_vader_sentidd_soft  0.863884   \n",
       "187  0.833143        ntusd_sentiwordnet_afinn_vader_sentidd_soft  0.864060   \n",
       "193  0.828571  sentiwordnet_stocktwitlexi_afinn_vader_sentidd...  0.864130   \n",
       "123  0.835429              ntusd_sentiwordnet_vader_sentidd_soft  0.865672   \n",
       "\n",
       "     Precision    Recall  \n",
       "201   0.779904  0.927894  \n",
       "198   0.775157  0.935484  \n",
       "107   0.790164  0.914611  \n",
       "160   0.790164  0.914611  \n",
       "200   0.776378  0.935484  \n",
       "203   0.777603  0.935484  \n",
       "199   0.793781  0.920304  \n",
       "148   0.792822  0.922201  \n",
       "202   0.818024  0.895636  \n",
       "184   0.816926  0.897533  \n",
       "126   0.820870  0.895636  \n",
       "190   0.822300  0.895636  \n",
       "76    0.846863  0.870968  \n",
       "127   0.848708  0.872865  \n",
       "94    0.823834  0.905123  \n",
       "132   0.824138  0.907021  \n",
       "130   0.827826  0.903226  \n",
       "187   0.848263  0.880455  \n",
       "193   0.826690  0.905123  \n",
       "123   0.851376  0.880455  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2_df.iloc[result2_df['F1_score'].sort_values().index].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.705426</td>\n",
       "      <td>senticnet_ntusd_sentiwordnet_stocktwitlexi_vad...</td>\n",
       "      <td>0.774108</td>\n",
       "      <td>0.700957</td>\n",
       "      <td>0.864307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.726098</td>\n",
       "      <td>ntusd_sentiwordnet_stocktwitlexi_vader_soft</td>\n",
       "      <td>0.774468</td>\n",
       "      <td>0.745902</td>\n",
       "      <td>0.805310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.706288</td>\n",
       "      <td>senticnet_ntusd_stocktwitlexi_afinn_sentidd_le...</td>\n",
       "      <td>0.774620</td>\n",
       "      <td>0.701796</td>\n",
       "      <td>0.864307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.716624</td>\n",
       "      <td>ntusd_sentiwordnet_stocktwitlexi_afinn_sentidd...</td>\n",
       "      <td>0.775733</td>\n",
       "      <td>0.721166</td>\n",
       "      <td>0.839233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.713178</td>\n",
       "      <td>ntusd_sentiwordnet_stocktwitlexi_hard</td>\n",
       "      <td>0.779616</td>\n",
       "      <td>0.707083</td>\n",
       "      <td>0.868732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.728682</td>\n",
       "      <td>ntusd_stocktwitlexi_vader_hard</td>\n",
       "      <td>0.782308</td>\n",
       "      <td>0.736021</td>\n",
       "      <td>0.834808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.724376</td>\n",
       "      <td>ntusd_sentiwordnet_stocktwitlexi_afinn_vader_l...</td>\n",
       "      <td>0.782313</td>\n",
       "      <td>0.726010</td>\n",
       "      <td>0.848083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.704565</td>\n",
       "      <td>senticnet_ntusd_stocktwitlexi_hard</td>\n",
       "      <td>0.782498</td>\n",
       "      <td>0.686318</td>\n",
       "      <td>0.910029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.716624</td>\n",
       "      <td>senticnet_ntusd_sentiwordnet_stocktwitlexi_sen...</td>\n",
       "      <td>0.783410</td>\n",
       "      <td>0.707491</td>\n",
       "      <td>0.877581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.751077</td>\n",
       "      <td>ntusd_afinn_soft</td>\n",
       "      <td>0.784167</td>\n",
       "      <td>0.794251</td>\n",
       "      <td>0.774336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.714040</td>\n",
       "      <td>senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...</td>\n",
       "      <td>0.785530</td>\n",
       "      <td>0.698851</td>\n",
       "      <td>0.896755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.735573</td>\n",
       "      <td>ntusd_stocktwitlexi_afinn_hard</td>\n",
       "      <td>0.786360</td>\n",
       "      <td>0.744401</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.749354</td>\n",
       "      <td>ntusd_stocktwitlexi_sentidd_hard</td>\n",
       "      <td>0.786500</td>\n",
       "      <td>0.782482</td>\n",
       "      <td>0.790560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.751938</td>\n",
       "      <td>ntusd_sentiwordnet_afinn_soft</td>\n",
       "      <td>0.786982</td>\n",
       "      <td>0.789318</td>\n",
       "      <td>0.784661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.761413</td>\n",
       "      <td>ntusd</td>\n",
       "      <td>0.790943</td>\n",
       "      <td>0.809892</td>\n",
       "      <td>0.772861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.766581</td>\n",
       "      <td>ntusd_sentiwordnet_soft</td>\n",
       "      <td>0.798812</td>\n",
       "      <td>0.804185</td>\n",
       "      <td>0.793510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.746770</td>\n",
       "      <td>ntusd_stocktwitlexi_soft</td>\n",
       "      <td>0.800813</td>\n",
       "      <td>0.740602</td>\n",
       "      <td>0.871681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.749354</td>\n",
       "      <td>ntusd_stocktwitlexi_afinn_soft</td>\n",
       "      <td>0.801906</td>\n",
       "      <td>0.744627</td>\n",
       "      <td>0.868732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.753661</td>\n",
       "      <td>ntusd_sentiwordnet_stocktwitlexi_soft</td>\n",
       "      <td>0.805442</td>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.873156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.755383</td>\n",
       "      <td>ntusd_sentiwordnet_stocktwitlexi_afinn_soft</td>\n",
       "      <td>0.806276</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.871681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy                                         Experiment  F1_score  \\\n",
       "138  0.705426  senticnet_ntusd_sentiwordnet_stocktwitlexi_vad...  0.774108   \n",
       "119  0.726098        ntusd_sentiwordnet_stocktwitlexi_vader_soft  0.774468   \n",
       "156  0.706288  senticnet_ntusd_stocktwitlexi_afinn_sentidd_le...  0.774620   \n",
       "183  0.716624  ntusd_sentiwordnet_stocktwitlexi_afinn_sentidd...  0.775733   \n",
       "59   0.713178              ntusd_sentiwordnet_stocktwitlexi_hard  0.779616   \n",
       "69   0.728682                     ntusd_stocktwitlexi_vader_hard  0.782308   \n",
       "180  0.724376  ntusd_sentiwordnet_stocktwitlexi_afinn_vader_l...  0.782313   \n",
       "31   0.704565                 senticnet_ntusd_stocktwitlexi_hard  0.782498   \n",
       "141  0.716624  senticnet_ntusd_sentiwordnet_stocktwitlexi_sen...  0.783410   \n",
       "15   0.751077                                   ntusd_afinn_soft  0.784167   \n",
       "135  0.714040  senticnet_ntusd_sentiwordnet_stocktwitlexi_afi...  0.785530   \n",
       "67   0.735573                     ntusd_stocktwitlexi_afinn_hard  0.786360   \n",
       "71   0.749354                   ntusd_stocktwitlexi_sentidd_hard  0.786500   \n",
       "60   0.751938                      ntusd_sentiwordnet_afinn_soft  0.786982   \n",
       "1    0.761413                                              ntusd  0.790943   \n",
       "13   0.766581                            ntusd_sentiwordnet_soft  0.798812   \n",
       "14   0.746770                           ntusd_stocktwitlexi_soft  0.800813   \n",
       "66   0.749354                     ntusd_stocktwitlexi_afinn_soft  0.801906   \n",
       "58   0.753661              ntusd_sentiwordnet_stocktwitlexi_soft  0.805442   \n",
       "118  0.755383        ntusd_sentiwordnet_stocktwitlexi_afinn_soft  0.806276   \n",
       "\n",
       "     Precision    Recall  \n",
       "138   0.700957  0.864307  \n",
       "119   0.745902  0.805310  \n",
       "156   0.701796  0.864307  \n",
       "183   0.721166  0.839233  \n",
       "59    0.707083  0.868732  \n",
       "69    0.736021  0.834808  \n",
       "180   0.726010  0.848083  \n",
       "31    0.686318  0.910029  \n",
       "141   0.707491  0.877581  \n",
       "15    0.794251  0.774336  \n",
       "135   0.698851  0.896755  \n",
       "67    0.744401  0.833333  \n",
       "71    0.782482  0.790560  \n",
       "60    0.789318  0.784661  \n",
       "1     0.809892  0.772861  \n",
       "13    0.804185  0.793510  \n",
       "14    0.740602  0.871681  \n",
       "66    0.744627  0.868732  \n",
       "58    0.747475  0.873156  \n",
       "118   0.750000  0.871681  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result3_df.iloc[result3_df['F1_score'].sort_values().index].tail(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
