{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "import html\n",
    "import re\n",
    "import string\n",
    "import preprocessor as p\n",
    "\n",
    "p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.SMILEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_1_FILEPATH = [\"//data/data1/training_set.json\", \"/data/data1/test_set.json\"]\n",
    "DATA_2_FILEPATH = \"/data/data2/tweets_labelled_09042020_16072020.csv\"\n",
    "DATA_3_FILEPATH = \"/data/data3/data3_final.csv\"\n",
    "DATA_4_FILEPATH = [\"/data/data4/stocktwits_data_ALL.csv\", \"/data/data4/stocktwits_data_ALL_cleaned.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(stocktwit_df, col_name):\n",
    "    def remove_emoji(tweets):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   u\"\\U0001f926-\\U0001f937\"\n",
    "                                   u\"\\U00010000-\\U0010ffff\"\n",
    "                                   u\"\\u2640-\\u2642\"\n",
    "                                   u\"\\u2600-\\u2B55\"\n",
    "                                   u\"\\u200d\"\n",
    "                                   u\"\\u23cf\"\n",
    "                                   u\"\\u23e9\"\n",
    "                                   u\"\\u231a\"\n",
    "                                   u\"\\ufe0f\"  # dingbats\n",
    "                                   u\"\\u3030\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', tweets)\n",
    "\n",
    "\n",
    "    stocktwit_df[col_name] = stocktwit_df[col_name].astype(str)\n",
    "\n",
    "    # Removing all tickers from comments\n",
    "    stocktwit_df[col_name] = stocktwit_df[col_name].str.replace(r'([$][a-zA-z\\.]{1,8})', '')\n",
    "\n",
    "    # Make all sentences small letters\n",
    "    stocktwit_df[col_name] = stocktwit_df[col_name].str.lower()\n",
    "\n",
    "    # Converting HTML to UTF-8\n",
    "    stocktwit_df[col_name] = stocktwit_df[col_name].apply(html.unescape)\n",
    "\n",
    "    # Removing hastags, mentions, pagebreaks, handles\n",
    "    # Keeping the words behind hashtags as they may provide useful information about the comments e.g. #Bullish #Lambo\n",
    "    stocktwit_df[col_name] = stocktwit_df[col_name].str.replace(r'(@[^\\s]+|[#]|[$])', ' ')  # Replace '@', '$' and '#...'\n",
    "    stocktwit_df[col_name] = stocktwit_df[col_name].str.replace(r'(\\n|\\r)', ' ')  # Replace page breaks\n",
    "\n",
    "    # Removing https, www., any links etc\n",
    "    stocktwit_df[col_name] = stocktwit_df[col_name].str.replace(r'((https:|http:)[^\\s]+|(www\\.)[^\\s]+)', ' ')\n",
    "\n",
    "    # Removing all numbers\n",
    "    stocktwit_df[col_name] = stocktwit_df[col_name].str.replace(r'[\\d]', '')\n",
    "\n",
    "    # Remove emoji\n",
    "    stocktwit_df[col_name] = stocktwit_df[col_name].apply(lambda row: remove_emoji(row))\n",
    "\n",
    "    # Remove punctuations\n",
    "    stocktwit_df[col_name] = stocktwit_df[col_name].str.translate(str.maketrans('', '', string.punctuation+\"“”‘’…•—\"))\n",
    "\n",
    "    # All additional cleaning\n",
    "    stocktwit_df[col_name] = stocktwit_df[col_name].apply(lambda row: p.clean(row))\n",
    "\n",
    "    # Remove whitespaces\n",
    "    stocktwit_df[col_name] = stocktwit_df[col_name].apply(lambda row: \" \".join(row.split()))\n",
    "\n",
    "    # Remove empty rows\n",
    "    stocktwit_df = stocktwit_df[stocktwit_df[col_name].str.contains(r'^\\s*$') == False]\n",
    "\n",
    "    return stocktwit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word= ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', '@', '#']\n",
    "\n",
    "def to_tokens(data):\n",
    "    sentence_token = [s.split(' ') for s in data] \n",
    "    return sentence_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data1(mode, to_clean= True):\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    train_path = cwd + DATA_1_FILEPATH[0]\n",
    "    test_path = cwd + DATA_1_FILEPATH[1]\n",
    "\n",
    "    with open(train_path, \"r\") as f:\n",
    "        data = f.read()\n",
    "        data1 = json.loads(data)\n",
    "\n",
    "    with open(test_path, \"r\") as f:\n",
    "        data = f.read()\n",
    "        test_data = json.loads(data)\n",
    "\n",
    "    data1.extend(test_data)\n",
    "    data1 = pd.DataFrame(data1)\n",
    "    \n",
    "    if(to_clean):\n",
    "        data1 = preprocess_data(pd.DataFrame(data1), \"tweet\")\n",
    "        \n",
    "    if(mode==\"dataframe\"):\n",
    "        data1_X = [item.lower() for item in data1['tweet']]\n",
    "        data1_y_class = [1 if float(item)>0 else 0 for item in data1[\"sentiment\"]]\n",
    "        \n",
    "        data1 = pd.DataFrame()\n",
    "        data1[\"text_cleaned\"] = data1_X\n",
    "        data1[\"Label\"] = data1_y_class\n",
    "        return data1\n",
    "    \n",
    "    elif(mode==\"list\"):\n",
    "        data1_X = [item.lower() for item in data1['tweet']]\n",
    "        data1_X = to_tokens(data1_X)\n",
    "        data1_y_class = [1 if float(item)>0 else 0 for item in data1[\"sentiment\"]]\n",
    "\n",
    "        return data1_X, data1_y_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data2(mode, to_clean= True):\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    path = cwd + DATA_2_FILEPATH\n",
    "    \n",
    "    data2 = pd.read_csv(path, sep=\";\")\n",
    "    data2 = data2[data2['sentiment'].notna()]\n",
    "    data2 = data2[data2['sentiment']!='neutral']\n",
    "    \n",
    "    if(to_clean):\n",
    "        data2 = preprocess_data(data2, \"text\")\n",
    "    \n",
    "    if(mode==\"dataframe\"):\n",
    "        data2_X = [item.lower() for item in data2['text']]\n",
    "        data2_y_class = [1 if item==\"positive\" else 0 for item in data2[\"sentiment\"]]\n",
    "        \n",
    "        data2 = pd.DataFrame()\n",
    "        data2[\"text_cleaned\"] = data2_X\n",
    "        data2[\"Label\"] = data2_y_class\n",
    "        return data2\n",
    "    \n",
    "    elif(mode==\"list\"):\n",
    "        data2_X = [item.lower() for item in data2['text']]\n",
    "        data2_X = to_tokens(data2_X)\n",
    "        data2_y_class = [1 if item==\"positive\" else 0 for item in data2[\"sentiment\"]]\n",
    "        \n",
    "        return data2_X, data2_y_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data3(mode, to_clean= True):\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    path = cwd + DATA_3_FILEPATH\n",
    "    \n",
    "    data3 = pd.read_csv(path)\n",
    "    \n",
    "    if(to_clean):\n",
    "        data3 = preprocess_data(data3, \"Message\")\n",
    "        \n",
    "    if(mode==\"dataframe\"):\n",
    "        data3_X = [item.lower() for item in data3['Message']]\n",
    "        data3_y_class = [int(sentiment) for sentiment in data3['Annotator1']]\n",
    "        \n",
    "        data3 = pd.DataFrame()\n",
    "        data3[\"text_cleaned\"] = data3_X\n",
    "        data3[\"Label\"] = data3_y_class\n",
    "        return data3\n",
    "    \n",
    "    elif(mode==\"list\"):\n",
    "        data3_X = [item.lower() for item in data3['Message']]\n",
    "        data3_X = to_tokens(data3_X)\n",
    "        data3_y_class = [int(sentiment) for sentiment in data3['Annotator1']]\n",
    "        \n",
    "        return data3_X, data3_y_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data4(mode, to_clean= True, sample_size=10000, seed=100):\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    if(to_clean):\n",
    "        path = cwd + DATA_4_FILEPATH[1]\n",
    "    else:\n",
    "        path = cwd + DATA_4_FILEPATH[0]\n",
    "        \n",
    "    data4 = pd.read_csv(path)\n",
    "    \n",
    "    if(mode==\"lexicon\"):\n",
    "        return data4\n",
    "    \n",
    "    elif(mode==\"bert\"):\n",
    "        data4['Label'] = [1 if sentiment==\"Bullish\" else 0 for sentiment in data4['Sentiment']]\n",
    "        data4['text_cleaned'] = data4['Message']\n",
    "        data4 = data4[[\"text_cleaned\", \"Label\"]]\n",
    "        \n",
    "        data4 = data4.sample(n=sample_size)\n",
    "        train_df = data4.sample(frac=0.8,random_state=seed)\n",
    "        val_df = data4.drop(train_df.index)\n",
    "        return train_df, val_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
